# üìÑ Documenta√ß√£o do Fluxo e Reuso ‚Äì AWS EC2 Pricing Pipeline

## üìå 1. Vis√£o Geral
Este projeto implementa um **pipeline h√≠brido** para coleta, tratamento, an√°lise e modelagem de pre√ßos de inst√¢ncias EC2 utilizando a **AWS Pricing API**.  
O fluxo separa a etapa de **ingest√£o pr√©-tratada** (script Python) da **an√°lise explorat√≥ria e modelagem** (notebook), garantindo **reprodutibilidade, flexibilidade e escalabilidade**.

üìä **Diagrama do Fluxo:**
![Fluxo do Pipeline](diagramas/fluxo_coleta_analise_aws.png)

---

## üìå 2. Pr√©-requisitos

### **Bibliotecas Python**
```bash
pip install boto3 pandas matplotlib seaborn joblib graphviz scikit-learn
```

### **Credenciais AWS**
Configurar com:
```bash
aws configure
```
Ou exportar vari√°veis de ambiente:
```bash
export AWS_ACCESS_KEY_ID=SEU_ACCESS_KEY
export AWS_SECRET_ACCESS_KEY=SEU_SECRET_KEY
export AWS_DEFAULT_REGION=us-east-1
```

### **Estrutura de Pastas**
```
data/        # Armazena CSVs de dados
diagramas/   # Diagramas e imagens do fluxo
modelos/     # Modelos treinados (.pkl)
notebooks/   # Notebooks de an√°lise
```

---

## üìå 3. Execu√ß√£o do Pipeline

### **Etapa 1 ‚Äì Coleta de Dados**
- Script: `extrair_precos_ec2.py`
- Fun√ß√£o: consultar a AWS Pricing API para inst√¢ncias EC2 (Linux/Shared) e gerar `custos_aws_ec2_on_demand.csv`.
- Comando:
```bash
python extrair_precos_ec2.py
```

### **Etapa 2 ‚Äì An√°lise Explorat√≥ria**
- Notebook: `notebooks/analise_precos_ec2.ipynb`
- Passos:
  1. Carregar `custos_aws_ec2_on_demand.csv`.
  2. Criar m√©tricas adicionais (EX: `price_per_vcpu`, `price_per_gb`).
  3. Gerar visualiza√ß√µes e insights.

### **Etapa 3 ‚Äì Modelagem**
- Fun√ß√£o de treino: `treinar_avaliar(df, nome_modelo)`
- Cen√°rios:
  - **Modelo sem Outliers** ‚Üí foco em inst√¢ncias de uso comum.
  - **Modelo com Outliers** ‚Üí inclui inst√¢ncias especializadas.
- Salvando modelos:
```python
import joblib
joblib.dump(modelo_normal, "modelos/modelo_normal.pkl")
joblib.dump(modelo_outlier, "modelos/modelo_outlier.pkl")
```

### **Etapa 4 ‚Äì Salvando Resultados**
- M√©tricas:
```python
salvar_metricas(met_normal, met_outlier, path="data/resultados.csv")
```
- Insights:
  - Salvar em `insights.md` para documentar achados.

---

## üìå 4. Par√¢metros e Configura√ß√£o
- **ServiceCode**: `"AmazonEC2"`
- **Filtros**:
  - `operatingSystem = Linux`
  - `tenancy = Shared`
  - `preInstalledSw = NA`
  - `capacitystatus = Used`
- **Regi√µes AWS**: configuradas no argumento `--regions` no script de coleta.
- **Pagina√ß√£o**: `max_pages` define o n√∫mero m√°ximo de p√°ginas coletadas.

---

## üìå 5. Reuso do Pipeline

### Alterar Servi√ßo AWS
Basta mudar `ServiceCode` no script:
```python
"ServiceCode": "AmazonRDS"  # Exemplo para RDS
```

### Alterar Filtros
Modificar lista `base_filters` para ajustar o tipo de inst√¢ncia desejado (ex.: GPU, Windows).

### Reutilizar Modelo Treinado
```python
import joblib
modelo = joblib.load("modelos/modelo_normal.pkl")
y_pred = modelo.predict(novos_dados)
```

---

## üìå 6. Pr√≥ximos Passos
- **Automatiza√ß√£o**:
  - AWS Lambda + EventBridge para rodar em hor√°rios definidos.
  - Cronjob em servidor/VPS.
- **Persist√™ncia**:
  - Salvar dados em banco (PostgreSQL, DynamoDB) para hist√≥rico.
- **Visualiza√ß√£o**:
  - Criar dashboard (Power BI, Grafana, Streamlit).
- **Modelos Avan√ßados**:
  - Utilizar **XGBoost** para tratar outliers de forma robusta.
  - Criar modelos separados para **uso comum** e **inst√¢ncias de nicho**.

---

## üìå 7. Contato
Para d√∫vidas ou melhorias, entre em contato com o autor do projeto.