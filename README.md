# ğŸ“„ DocumentaÃ§Ã£o do Fluxo e Reuso â€“ AWS EC2 Pricing Pipeline

## ğŸ“Œ 1. VisÃ£o Geral
Este projeto implementa um **pipeline hÃ­brido** para coleta, tratamento, anÃ¡lise e modelagem de preÃ§os de instÃ¢ncias EC2 utilizando a **AWS Pricing API**.  
O fluxo separa a etapa de **ingestÃ£o prÃ©-tratada** (script Python) da **anÃ¡lise exploratÃ³ria e modelagem** (notebook), garantindo **reprodutibilidade, flexibilidade e escalabilidade**.

ğŸ“Š **Diagrama do Fluxo:**

![Fluxo do Pipeline](diagramas/fluxo_coleta_analise_aws.png)

---

## ğŸ“Œ 2. PrÃ©-requisitos

### **Bibliotecas Python**
```bash
pip install boto3 pandas matplotlib seaborn joblib scikit-learn
```

### **Credenciais AWS**
Configurar com:
```bash
aws configure
```
Ou exportar variÃ¡veis de ambiente:
```bash
export AWS_ACCESS_KEY_ID=SEU_ACCESS_KEY
export AWS_SECRET_ACCESS_KEY=SEU_SECRET_KEY
export AWS_DEFAULT_REGION=us-east-1
```

### **Estrutura de Pastas**
```
data/        # Armazena CSVs de dados
diagramas/   # Diagramas e imagens do fluxo
modelos/     # Modelos treinados (.pkl)
notebooks/   # Notebooks de anÃ¡lise
```

---

## ğŸ“Œ 3. ExecuÃ§Ã£o do Pipeline

### **Etapa 1 â€“ Coleta de Dados**
- Script: `extrair_precos_ec2.py`
- FunÃ§Ã£o: consultar a AWS Pricing API para instÃ¢ncias EC2 (Linux/Shared) e gerar `custos_aws_ec2_on_demand.csv`.
- Comando:
```bash
python extrair_precos_ec2.py
```

### **Etapa 2 â€“ AnÃ¡lise ExploratÃ³ria**
- Notebook: `analise_precos_ec2.ipynb`
- Passos:
  1. Carregar `custos_aws_ec2_on_demand.csv`.
  2. Criar mÃ©tricas adicionais (EX: `price_per_vcpu`, `price_per_gb`).
  3. Gerar visualizaÃ§Ãµes e insights.

### **Etapa 3 â€“ Modelagem**
- FunÃ§Ã£o de treino: `treinar_avaliar(df, nome_modelo)`
- CenÃ¡rios:
  - **Modelo sem Outliers** â†’ foco em instÃ¢ncias de uso comum.
  - **Modelo com Outliers** â†’ inclui instÃ¢ncias especializadas.
- Salvando modelos:
```python
import joblib
joblib.dump(modelo_normal, "modelos/modelo_normal.pkl")
joblib.dump(modelo_outlier, "modelos/modelo_outlier.pkl")
```

### **Etapa 4 â€“ Salvando Resultados**
- MÃ©tricas:
```python
salvar_metricas(met_normal, met_outlier, path="data/resultados.csv")
```
- Insights:
  - Salvar em `insights.md` para documentar achados.

---

## ğŸ“Œ 4. ParÃ¢metros e ConfiguraÃ§Ã£o
- **ServiceCode**: `"AmazonEC2"`
- **Filtros**:
  - `operatingSystem = Linux`
  - `tenancy = Shared`
  - `preInstalledSw = NA`
  - `capacitystatus = Used`
- **RegiÃµes AWS**: configuradas no argumento `--regions` no script de coleta.
- **PaginaÃ§Ã£o**: `max_pages` define o nÃºmero mÃ¡ximo de pÃ¡ginas coletadas.

---

## ğŸ“Œ 5. Reuso do Pipeline

### Alterar ServiÃ§o AWS
Basta mudar `ServiceCode` no script:
```python
"ServiceCode": "AmazonRDS"  # Exemplo para RDS
```

### Alterar Filtros
Modificar lista `base_filters` para ajustar o tipo de instÃ¢ncia desejado (ex.: GPU, Windows).

### Reutilizar Modelo Treinado
```python
import joblib
modelo = joblib.load("modelos/modelo_normal.pkl")
y_pred = modelo.predict(novos_dados)
```

---

## ğŸ“Œ 6. PrÃ³ximos Passos
- **AutomatizaÃ§Ã£o**:
  - AWS Lambda + EventBridge para rodar em horÃ¡rios definidos.
  - Cronjob em servidor/VPS.
- **PersistÃªncia**:
  - Salvar dados em banco (PostgreSQL, DynamoDB) para histÃ³rico.
- **VisualizaÃ§Ã£o**:
  - Criar dashboard (Power BI, Grafana, Streamlit).
- **Modelos AvanÃ§ados**:
  - Utilizar **XGBoost** para tratar outliers de forma robusta.
  - Criar modelos separados para **uso comum** e **instÃ¢ncias de nicho**.

---

## ğŸ“Œ 7. DÃºvidas ou melhorias

## **ğŸ’¬ Contato**  

ğŸ“§ **Email:** borgesmatheus1201@gmail.com  
ğŸ **GitHub:** [borges12matheus](https://github.com/borges12matheus)  
ğŸ”— **LinkedIn:** [matheusborges12](https://www.linkedin.com/in/matheusborges12/)
